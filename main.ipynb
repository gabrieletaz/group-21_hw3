{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collector\n",
    "we have a function called generate_html() that collects all the html files.\n",
    "we loop through a list of 3 urls and grab 30_000 urls and put them in a list called movie_list.\n",
    "for every url we grab all anchor tag ('a',href=True) and then we append the value which is link['href'] to the list mentioned above.\n",
    "In the next round we grab every url from the above list. which is a string and in order to parse the html page we pass it to beautifulsoup and then we save them into different files. like article-0.html\n",
    "one trick we do here is we add an anchor tag which basically is the link of the movie to the end of html file with the class='group-21'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "in this part we loop through every part using f-string method and we open every file to grab the info required.\n",
    "first we grab the table . and then we get the title wich is h1. for the intro part we select .mw-parser-output\n",
    "and call recursiveChildGenerator to get the P till we get h2.\n",
    "for the plot we get Plot_Summary and to the same thing basically.\n",
    "we put every thing we get and put inside the wrapping function. NA for missing info. and put them all inside tsv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "In this function we open each tsv files. and we remove these\n",
    "Stop Words\n",
    "Punctuations\n",
    "Stem the words\n",
    "and remove accents\n",
    "then we use this preprocessed words to create vocabulary. which is a json file named vocabulary.json with this structure\n",
    "words: index words are the keys and index is the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index1 & 2\n",
    "for index 1 we created a dict with index of a word as a key and a list of document ids which contains the word.\n",
    "for index 2 we created a dict with index of the word as a key and a list of document_ids and its tfidf with the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "we calculate the tfidf vector of the query, and the tfidf of each document with the query. then we apply the formula of the cosine similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score for the 3rd part.\n",
    "We used the absolute difference between the duration time and the preferred by the user and the duration time of each document. and we rank the documents using this score. in this way the user can decide which film is the best if he/she has limitted time to watch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo question\n",
    "The problem can be solved with a recursive algorithm but it would have a exponential complexity so the optimal solution is using dynamic programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a dictionary stored in the memory in which each actor is associated to all the movies in which he starred. Then we defined functions to add nodes and edges to the graph and also to improve its visualization.Therefore, we can ask the user if he wants to see the CO-STARDOM network and, in case of positive answer, we can show him the graph generated by the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
